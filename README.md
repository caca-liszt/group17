# Fine-Tuning LLMs with RLHF for Bias Reduction in Resume Screening

LLMs are increasingly used in resume screening to reduce the workload of recruiters. However, LLMs often inherit and reproduce historical societal biases related to gender, race, ethnicity, and other demographic factors. This inherent bias in LLMs can lead to unfair outcomes. A prominent example is Amazon's AI recruiting tool, which was found to be biased against women (Dastin, 2018). According to recent research from the University of Washington, LLMs favored White-associated names 85.1\% of the time over Black-associated names (8.6\%) and were also biased against female-associated names (Wilson \& Caliskan, 2024). Recognizing this critical issue, our project focuses on methodologies to ensure fairness and equity in AI hiring tools.

Our research is guided by the question: "To what extent can RLHF)effectively reduce demographic biases in LLM-based resume screening systems while maintaining high accuracy in identifying qualified candidates?" Ensuring fairness has significant ethical and practical importance. A biased hiring process not only harms the candidates themselves but also causes employers to miss out on diverse talent.

Our topic is unique as it employs RLHF as a technique for bias mitigation. While other algorithmic debiasing methods exist, RLHF integrates human values into the model. This allows it to detect more subtle forms of bias.

## Members

- [Jasmine Zhu](https://github.com/jasminezjr) 
- [Yuxuan Qin](https://github.com/caca-liszt) 
- [Cynthia Cui](https://github.com/yc6062-rgb) 

## Database


## Baseline

Run `baseline.ipynb`

## Baseline Evaluation

Run `baseline_fairness.ipynb`

## Human Feedback

Run `human_feedback_empty.ipynb`

## RLHF


## RLHF Evaluation

Run `rlhf_fairness`
